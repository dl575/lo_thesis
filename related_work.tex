\chapter{Related Work}
\label{chap:related_work}

This chapter describes some of the previous work that is related to this
thesis. Section~\ref{sec:related_work.realtime} discusses related work in
hardware design and WCET analysis of real-time systems.
Section~\ref{sec:related_work.security} talks about related work on security and
reliability, specifically previous work on run-time monitoring and partial
monitoring. Finally, Section~\ref{sec:related_work.energy} discusses work
related to our techniques for prediction-based DVFS control including previous
DVFS control techniques and methods for execution time prediction.

\section{Real-Time Systems}
\label{sec:related_work.realtime}

\subsection{Hardware Architectures for Real-Time Systems}

Previous work has looked at designing processor architectures for real-time
systems. These projects focus on improving performance in predictable ways in
order to improve the worst-case execution time. For example, several projects
\cite{spear-ecrts03, mcgrep-rtss06, jop-jsa07, patmos-ppes2011}
have created architectures with simple pipelines that allow for static
scheduling of instructions. These architectures are easily analyzable so that the
resulting WCET estimates are tight and lower than more unpredictable architectures. Other projects
have explored using fine-grained simultaneous multi-threading (SMT) in order to
improve performance by isolating hard real-time threads from each other
\cite{pret-dac07, pret-cases08, ptarm-iccd12} or from soft or non-real-time
threads \cite{merasa-micro10, carcores-arcs10, flexpret-rtas14}. Whitham et al.
explored reducing the unpredictability of control flow by co-designing software
and hardware to execute traces \cite{whitham-comp10}. Finally, the Virtual
Simple Architecture (VISA) \cite{visa-isca03, multi_task_visa-rtss04} uses
dynamic slack to run in a faster but difficult to analyze mode. It switches to
a simpler mode under which timing analysis and WCET is calculated when there is
not enough slack. This is similar to our use of slack to opportunistically
perform monitoring but in the context of improving performance.
These architecture all look to improve on the analyzable worst-case
performance. Instead, our work has looked at designing hardware architectures
to improve on security, reliability, and energy-efficiency in the context of
real-time deadlines.

%There has been previous work at building hardware targeted at real-time
%systems. All of this work looks at how techniques for improving the performance
%of processors can be applied to real-time systems. Instead, in this thesis, we
%have explored how techniques for security and energy-efficiency can be adapted
%for real-time systems.
%
%Virtual Simple Architecture (VISA) \cite{visa-isca03,
%multi_task_visa-rtss04} proposes an architecture that combines a simple and
%real-time analyzable mode of operation with a higher-performance mode
%without guarantees. The system runs in the high-performance mode as long as
%there is slack and switches to the simple mode if slack runs out in order to
%guarantee that deadlines are met.

%FlexPRET \cite{flexpret-rtas14}, Precision-Timed ARM (PTARM) \cite{ptarm-iccd12}, PRET \cite{pret-cases08, pret-dac07}
%The Precision Timed (PRET) Machines project \cite{pret-dac07, pret-cases08,
%ptarm-iccd12, flexpret-rtas14} takes advantage of fine-grained multi-threading
%in order to improve system performance while isolating threads in order to
%allow each thread's performance to be easily analyzable.

%Patmos \cite{patmos-ppes2011} is a dual-issue RISC processor that achieves
%tight WCET bounds through static scheduling of instructions and modifications
%to the memory hierarchy for predictability. This requires compiler support in
%order to be effective.

% The Java Optimized Processor (JOP) \cite{jop-jsa07} is a 4-stage pipelined
% processor that runs Java bytecode. Microcode instructions always take one cycle
% so WCET analysis for the JOP depends only control flow and not pipeline or data
% dependencies.
% 
% The Microprogrammed Coarse Grained REconfigurable Processor (MCGREP)
% \cite{mcgrep-rtss06} uses a simple two-stage pipeline with two execution units
% for predictability. By microprogramming the two execution units, in a manner
% similar to VLIW, they are able to achieve predictable speedup due to
% parallelism, the lack of decode needed for microcode, and fast on-chip
% microcode store. 
% 
% CarCores \cite{carcores-arcs10} uses simultaneous multi-threading (SMT) to
% allow a highest priority thread to run with same execution time as in non-SMT
% and thus be predictable. Additional non-real time threads can be schedule for
% improved system utilization but do not have guarantees.
% 
% %Multi-core system with shared bus/memory \cite{paolieri-isca09}
% 
% Whitham et al. \cite{whitham-comp10} take advantage of the fact that if a
% pipeline is flushed at the beginning of a basic block, then execution time is
% deterministic. They use the idea of virtual traces in order to create
% effectively larger basic blocks to take advantage of this.
% 
% SPEAR \cite{spear-ecrts03} presents a predictable processor architecture by
% using a simple 3-stage pipelined in-order processor that includes predication
% and hardware support for predictable interrupt handling (exception vector
% table).
% 
% The Merasa Project \cite{merasa-micro10} explored a design of a multi-core
% processor for real-time systems. Each core is 4-way SMT with one hard real-time
% thread that has priority and 3 non-hard real-time threads. The hard real-time
% thread uses scratchpads while the other threads use caches. To handle
% multi-core interference, they use a shared bus that has bounded delay for hard
% real-time threads, dynamic cache partitioning, and an analyzable memory
% controller.

\subsection{Worst-Case Execution Time Analysis}

Estimating the worst-case execution time of a sequential program on a
single-core system is a well studied problem. A survey paper by Wilhelm et al.
\cite{wcetsurvey-tecs08} provides an overview of existing methods and tools in
this context.  However, to the best of our knowledge, we were the
first to study the WCET of parallel monitoring.  Researchers have recently
started studying the WCET problem for multi-core systems. For example,
Paolieri et al. proposed a multi-core hardware architecture for hard real-time
systems and analyzed its WCET behavior \cite{paolieri-isca09}.  McAiT is a tool
that has been developed for WCET analysis of multi-core real-time software
\cite{mcait-rtss10}. Chattopadhyay et al. developed an ILP-based approach for
multi-core WCET analysis including shared caches and buses
\cite{chattopadhyay-rtas12}. These studies focused on the contention between
parallel programs for shared resources such as memory. However, the loosely
coupled link between the main core and parallel monitoring hardware represents
a producer-consumer relationship rather than shared resources. Thus, we found
that previously developed techniques were not directly applicable or easily
adaptable to provide a tight WCET bound for a system with parallel monitoring.

\section{Security and Reliability}
\label{sec:related_work.security}

\subsection{Run-Time Monitoring}

% Monitoring
In this thesis, we have explored applying run-time monitoring to real-time
systems. Our work has been designed to be applicable to a wide range of
parallel hardware run-time monitoring techniques.  We briefly mention some
recent parallel monitoring platforms as examples. For example, INDRA
\cite{indra-isca06} uses a checker core to monitor coarse-grained events on a
computation core such as function call/return, code origin inspection, and
control flow inspection.  Nagarajan et al. studied implementing DIFT on a
multi-core system \cite{nagarajan-interact08}.  Chen et al. proposed hardware
acceleration techniques for multi-core systems and showed that a set of
parallel monitoring techniques for security and software debugging can be
realized with low performance overheads (tens of percents) \cite{lba-isca08}.
The FlexCore \cite{flexcore-micro10} and Harmoni \cite{harmoni-dsn12}
architectures showed that parallel monitoring can be made even more efficient
by implementing monitors on reconfigurable hardware. SecureCore
\cite{yoon-securecore-rtas13} is a monitoring scheme targeted specifically at
real-time systems which attempts to detect code injection attacks by detecting
anomalous timing behavior. However, the architecture assumes enough buffering
so that the timing behavior of the main task is not affected. Overall, these
previous studies demonstrate that parallel monitoring can be used to
significantly improve system security and reliability with minimal overheads.

\subsection{Partial Monitoring}

% Previous work on sampling for monitoring
\begin{table}[tb]
  \begin{center}
    \begin{footnotesize}
    \input{tabs/previous_sampling}
    \end{footnotesize}
    \caption{Previous work on partial run-time monitoring.}
    \label{tab:related.previous_sampling}
  \end{center}
\end{table}

There exists a number of previous projects that have looked into performing
partial monitoring in order to reduce the performance overhead. These platforms
differ from the architectures we have presented in
Chapter~\ref{chap:monitoring_hard_drop} and \ref{chap:monitoring_dift_drop} in a
number of ways including how monitoring is implemented and the monitoring
techniques targeted. However, we note three main properties that differentiates
our work:
\begin{enumerate}
  \item \textbf{Generality:} Our architectures apply to a variety of
  monitoring techniques.
  \item \textbf{Target Overhead:} Our architectures allow an overhead or
  deadline to be targeted. The architecture presented in
  Chapter~\ref{chap:monitoring_hard_drop} provides hard guarantees on the
  overhead while the architecture from Chapter~\ref{chap:monitoring_dift_drop}
  relaxes the strict guarantee. Pervious work performs sampling to reduce overhead
  but does not try to bound overhead, which we have shown can vary greatly.
  \item \textbf{Prevent False Positives:} We have presented an
  invalidation-based mechanism to prevent false positives. Previous work either
  has false positives or targets monitoring techniques which degrade gracefully
  with sampling rather than exhibit false positives.
\end{enumerate}
Our work is the first that we know of to present a hardware platform for
partial monitoring that is general, allows a deadline or overhead target to be
specified, and explicitly prevents false positives. In contrast, previous work
on partial monitoring achieves only some, but not all, of these properties.
Table~\ref{tab:related.previous_sampling} summarizes these differences. 

For example, there exists previous work for using statistical sampling to
reduce the performance overhead of various debugging techniques. These include
sampling for bug isolation \cite{liblit-pldi05}, memory leak detection
\cite{chilimbi-asplos04}, race detection \cite{literace-pldi09, pacer-pldi10},
and information flow tracking \cite{testudo-micro08, greathouse-cgo11}. These
techniques modify the monitoring to support statistical sampling and so are not
generalizable. For those that prevent false positives, this is also done with
monitor-specific modifications. Finally, with the exception of the work by
Greathouse et al. \cite{greathouse-cgo11}, they do not allow an overhead target
to be specified.

There also exists several projects that have looked into more general partial
monitoring platforms. Arnold and Ryder \cite{arnold-pldi01} presented a general
platform for sampling of instrumented code, but do not allow an overhead target
to be specified and do not prevent false positives. Huang et al.
\cite{huang-sttt12} proposed a general framework for controlling the overhead
of software-based monitoring. However, they also do not explicitly address
false positives and only target monitors which degrade gracefully when
performed partially. Finally, QVM \cite{qvm-oopsla08} proposes a modification
to the Java Virtual Machine (JVM) to support monitoring with adjustable
overhead. QVM is limited to monitoring for code run on a JVM which limits its
generalizability while our framework works for any binary. QVM prevents false
positives by enabling or disabling monitoring on a per-object basis. This
limits the monitoring schemes that can be implemented. Also, this method of
preventing false positives is similar to the idea of performing source-only
dropping (see Section~\ref{sec:monitoring_dift_drop.policies.which}) which our
results show can lead to overshooting the overhead target depending on the
monitoring technique. 

\section{Performance-Energy Trade-off}
\label{sec:related_work.energy}

\subsection{Dynamic Voltage and Frequency Scaling}

There have been many designs for DVFS controllers. Most controllers look to
decrease frequency when the performance impact is minimal. For example, the
built-in Linux governors \cite{linux_governors} adjust DVFS based on CPU
utilization.  However, these controllers do not take into account performance
requirements or deadlines.

DVFS has been studied in the context of hard real-time systems
\cite{rtdvfs-systor12}. For these systems, deadlines are strict requirements
that cannot be violated. Thus, lowering frequency must be done in a
conservative manner.  By relaxing this strict requirement, our prediction-based
controller is able to achieve higher energy savings.

A number of reactive DVFS controllers have been proposed that use the past
history of job execution times to predict the execution time of future jobs.
Choi et al. \cite{choi-iccad02} used moving averages of job execution time
history to predict execution times for an MPEG decoder. Similarly, Pegasus
\cite{pegasus-isca14} used instantaneous and average job execution times to
make DVFS decisions. Nachiappan et al. \cite{nachiappan-hpca15} used a moving
average to set DVFS for multiple IP cores. Gu and Chakraborty \cite{gu-dac08}
used a PID-based controller to predict execution times of frames in a game.
These history-based, reactive controllers are not able to adapt fast enough to
job-to-job variations in execution time, resulting in either high energy usage
or deadline misses. In fact, our results show that prediction-based control
outperforms PID-based control.

Prediction-based approaches have been designed for specific applications. Gu
and Chakraborty \cite{gu-rtas08} predicted the rendering time for a game frame
based on the number of objects in the scene. Zhu et al. used prediction-based
control to select core and DVFS levels for a web browser based on HTML and CSS
features \cite{zhu-hpca13} and event types \cite{eqos-hpca15}. Adrenaline
\cite{adrenaline-hpca15} looked to reduce the tail latency of datacenter
applications including web search and Memcached by classifying jobs by query
type. These predictive approaches required careful analysis of the applications
of interest in order to identify features and create predictive models.  Our
approach presents an automated approach to create these DVFS predictors for a
wide range of applications. For example, for the web browser we tested, our
approach automatically identifies event types as a feature because of changes
in control flow depending on event type.

\subsection{Execution Time Prediction}

Worst-case execution time analysis is a well-studied problem in hard real-time
systems \cite{wcetsurvey-tecs08}. 
%This analysis looks at the problem of
%estimating execution time of a program in the worst-case. 
This can be used as a
conservative bound for setting DVFS in order to meet deadlines, but it does not
predict the changes in job execution time based on specific inputs or program
state.
ATLAS \cite{atlas-rtas13} looked at predicting execution time in the context of
soft real-time scheduling. Their approach uses programmer-marked features in a
linear model in order to predict execution time. Instead, our approach is able
to automatically identify features without programmer assistance.  Mantis
\cite{mantis-atc13} presents an automated approach for predicting execution
time, similar to the approach we have presented. However, neither Mantis nor
ATLAS looks at execution time prediction in the context of DVFS control.
To apply execution time prediction to DVFS control, we needed to create a
prediction method that placed greater penalty on under-prediction and to extend
the predictor to select the appropriate frequency level.
