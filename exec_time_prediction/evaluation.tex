\section{Evaluation}
\label{sec:exec_time_prediction.evaluation}

\subsection{Experimental Setup}
\label{sec:exec_time_prediction.evaluation.setup}

% Benchmark descriptions
\begin{table*}
  \begin{center}
    \begin{footnotesize}
    \input{exec_time_prediction/tabs/benchmarks.tex}
    \end{footnotesize}
    \caption{Benchmark descriptions and task of interest.}
    \label{tab:exec_time_prediction.evaluation.benchmarks}
  \end{center}
\end{table*}

% Benchmark execution times
\begin{table*}
  \begin{center}
    \begin{footnotesize}
    \input{exec_time_prediction/tabs/job_statistics.tex}
    \end{footnotesize}
    \caption{Execution time statistics when running
    at maximum frequency.}
    \label{tab:exec_time_prediction.evaluation.job_statistics}
  \end{center}
\end{table*}

We applied our framework for prediction-based DVFS control to a set of eight
benchmark applications including three games, a web browser, speech
recognition, a video decoder and two applications from the MiBench suite
\cite{mibench}.  Table~\ref{tab:exec_time_prediction.evaluation.benchmarks}
lists and describes these benchmarks and the task of interest for each.
Table~\ref{tab:exec_time_prediction.evaluation.job_statistics} shows the
minimum, average, and maximum job execution times for these benchmarks when run
at maximum frequency.

We ran our experiments on an ODROID-XU3 \cite{odroid} development board running
Ubuntu 14.04. The ODROID-XU3 includes a Samsung Exynos5422 SoC with ARM
Cortex-A15 and Cortex-A7 cores. We show results here for running on the more
power-efficient A7 core but we saw similar trends when running on the A15 core.
In order to isolate measurements for the application of interest, we pinned the
benchmark to run on the A7 core while using the A15 to run OS and background
jobs. We measured power using the on-board power sensors with a sampling rate
of 213 samples per second and integrated over time to calculate energy usage.

We compare our proposed prediction-based DVFS controller with existing
controllers and previously proposed control schemes. Specifically, we measure
results for the following DVFS schemes:
\begin{enumerate}
  \item \textbf{performance}: The Linux performance governor
  \cite{linux_governors} always runs at the maximum frequency. We normalize our
  energy results to this case.
  \item \textbf{interactive}: The Linux interactive governor
  \cite{linux_governors} was designed for interactive mobile applications. It
  samples CPU utilization every 80 milliseconds and changes to maximum
  frequency if CPU utilization is above 85\%.
  \item \textbf{pid}: The PID-based controller uses previous prediction errors
  with a PID control algorithm in order to predict the execution time of the
  next job \cite{gu-dac08}. The PID parameters are trained offline and are
  optimized to reduce deadline misses.
  \item \textbf{prediction}: This is our prediction-based controller as
  described in this chapter.
\end{enumerate}

\subsection{Energy Savings and Deadline Misses}

% Energy, deadline misses
\begin{figure*}
  \begin{center}
    \includegraphics{exec_time_prediction/data/em_summary.pdf}
    \caption{Normalized energy usage and deadline misses.}
    \label{fig:exec_time_prediction.evaluation.em_summary}
  \end{center}
\end{figure*}

Figure~\ref{fig:exec_time_prediction.evaluation.em_summary} compares energy
consumption and deadline misses for the different DVFS controllers across our
benchmark set. These experiments are run with a time budget of 50 milliseconds per job as
running faster than this is not noticeable to a user \cite{lindgaard-bit06,
eqos-hpca15}.  pocketsphinx takes at least 100s of milliseconds to run (see
Table~\ref{tab:exec_time_prediction.evaluation.benchmarks}) so we use a 4
second deadline. This corresponds to the time limit that a user is willing to
wait for a response \cite{miller-afips68}.  Energy numbers are normalized to
the energy usage of the performance governor.  Deadline misses are reported as
the percentage of jobs that miss their deadline. 

We see that, on average, our prediction-based controller saves 56\% energy
compared to running at max frequency. This is 27\% more savings than the
interactive governor and 1\% more savings than the PID-based controller.  For
ldecode, pocketsphinx, and rijndael, prediction-based control shows higher
energy consumption than PID-based control. However, if we look at deadline
misses, we see that PID-based control shows a large number of misses for these
benchmarks. On average, the interactive governor shows 2\% deadline misses and
the PID-based controller shows 13\% misses. In contrast, our prediction-based
controller shows 0.1\% deadline misses for curseofwar and no deadline misses
for the other benchmarks tested. Overall, we see that the interactive governor
has a low number of deadline misses, but achieves this with high energy
consumption. On the other hand, PID-based control shows lower energy usage than
the prediction-based controller in some cases, but this comes at the cost of a
large number of deadline misses. Instead, on average, our prediction-based
control is able to achieve both better energy consumption and less deadline
misses than the interactive governor and PID-based control.

% Deadline sweeps
\begin{figure*}
  \begin{center}
  \begin{subfigure}[2048]{
    \includegraphics{exec_time_prediction/data/2048.pdf}
  }
  \end{subfigure}
  \hspace{-0.3in}
  \begin{subfigure}[curseofwar]{
    \includegraphics{exec_time_prediction/data/curseofwar.pdf}
  }
  \end{subfigure}

  \begin{subfigure}[ldecode]{
    \includegraphics{exec_time_prediction/data/ldecode.pdf}
  }
  \end{subfigure}
  \hspace{-0.3in}
  \begin{subfigure}[pocketsphinx]{
    \includegraphics{exec_time_prediction/data/pocketsphinx.pdf}
  }
  \end{subfigure}

  \begin{subfigure}[rijndael]{
    \includegraphics{exec_time_prediction/data/rijndael.pdf}
  }
  \end{subfigure}
  \hspace{-0.3in}
  \begin{subfigure}[sha]{
    \includegraphics{exec_time_prediction/data/sha.pdf}
  }
  \end{subfigure}

  \begin{subfigure}[uzbl]{
    \includegraphics{exec_time_prediction/data/uzbl.pdf}
  }
  \end{subfigure}
  \hspace{-0.3in}
  \begin{subfigure}[xpilot]{
    \includegraphics{exec_time_prediction/data/xpilot.pdf}
  }
  \end{subfigure}

  \caption{Normalized energy usage and deadline misses as time budget is
  varied.}
  \label{fig:exec_time_prediction.evaluation.sweeps}
  \end{center}
\end{figure*}

Since our prediction-based controller takes the time budget into account, it is
able to save more energy with longer time budgets. Similarly, with shorter time
budgets, it will spend more energy to attempt to meet the tighter deadlines.
In order to study this trade-off, we swept the time budget around the point
where we expect to start seeing deadline misses. Specifically, we set a
normalized budget of 1 to correspond to the maximum execution time seen for the
task when running at maximum frequency (see
Table~\ref{tab:exec_time_prediction.evaluation.benchmarks}). This corresponds
to the tightest budget such that all jobs are able to meet their deadline.
Figure~\ref{fig:exec_time_prediction.evaluation.sweeps} shows the energy usage
and deadline misses for the various benchmarks as the normalized budget is
swept below and above 1.  We see that our prediction-based controller is able
to save more energy with longer time budgets and continues to outperform the
interactive governor and the PID-based controller for varying time budgets.
For normalized budgets less than 1, our prediction-based controller shows
deadline misses. However, the number of misses is typically close to the number
seen with the performance governor.  This implies that the only deadline misses
are the ones that are impossible to meet at the specified time budget, even
with running at the maximum frequency.

\subsection{Analysis of Overheads and Error}
\label{sec:exec_time_prediction.evaluation.overheads}

% Overhead time
\begin{figure}
  \begin{center}
    \includegraphics{exec_time_prediction/data/overhead_time.pdf}
    \caption{Average time to run prediction slice and switch DVFS levels.}
    \label{fig:exec_time_prediction.evaluation.overhead_time}
  \end{center}
\end{figure}

Figure~\ref{fig:exec_time_prediction.evaluation.overhead_time} shows the
average times for executing the predictor and for switching DVFS levels.  On
average, the predictor takes 3.2 ms to execute and DVFS switching takes 0.8 ms.
Excluding pocketsphinx, the average total overhead is less than 1 ms which is
2\% of a 50 ms time budget.  pocketsphinx shows a longer execution time of 24 ms for the
predictor. However, this time is negligible compared to the execution time of
pocketsphinx jobs which are on the order of seconds.

% Results w/o overhead
\begin{figure}
  \begin{center}
    \includegraphics{exec_time_prediction/data/overhead_comparison.pdf}
    \caption{Normalized energy usage and deadline misses with overheads removed
    and oracle prediction.}
    \label{fig:exec_time_prediction.evaluation.overhead_comparison}
  \end{center}
\end{figure}

The overheads of executing the predictor and DVFS switching decrease the energy
savings achievable. This is due to the energy consumed to perform these
operations as well as the decrease in effective budget.  Better program slicing
and/or feature selection could reduce the predictor execution time.  Similarly,
faster DVFS switching circuits \cite{booster-hpca12, shortstop-vlsic13,
fgsync-micro14} have shown switching times on the order of tens of nanoseconds.
In order to explore the limits of what is achievable, we evaluated our
prediction-based control when the overheads of the predictor and DVFS switching
are ignored.
Figure~\ref{fig:exec_time_prediction.evaluation.overhead_comparison} shows the
energy and deadline misses when these overheads are removed. These results are
shown for a time budget of 4 seconds for pocketsphinx and 50 milliseconds for all other
benchmarks.  On average, we see a 3\% decrease in energy consumption when
removing the overheads of DVFS switching. Removing the overhead of running the
predictor shows negligible improvement past removing the DVFS switching
overhead. Thus, improvements to the program slicing and feature selection
execution time are expected to have little effect on energy savings.

% Prediction error
\begin{figure}
  \begin{center}
    \includegraphics{exec_time_prediction/data/prediction_error.pdf}
    \caption{Box-and-whisker plots of prediction error. Positive number
    correspond to over-prediction and negative numbers correspond to
    under-prediction.}
    \label{fig:exec_time_prediction.evaluation.prediction_error}
  \end{center}
\end{figure}

In addition to these overheads, the accuracy of our prediction limits the
effectiveness of the prediction-based controller.
Figure~\ref{fig:exec_time_prediction.evaluation.prediction_error} shows
box-and-whisker plots of the prediction error.  The box indicates the first and
third quartiles and the line in the box marks the median value. Outliers
(values over 1.5 times the inner quartile range past the closest box end) are
marked with an ``x'' and the non-outlier range is marked by the whiskers.
Positive values represent over-prediction and negative-numbers represent
under-prediction. We can see that the prediction skews toward over-prediction
with average errors greater than 0.  Most benchmarks show prediction errors of
less than 5 ms, which is 10\% of a 50 ms time budget. ldecode and rijndael
show higher prediction errors, which limits the energy savings possible.
pocketsphinx (not shown) has errors ranging from 60 ms under-prediction to 2
seconds over-prediction with an average of 880 ms over-prediction. Although
these errors are larger in absolute terms than the other benchmarks, they are
on the same order of magnitude as seen for the other benchmarks when 
compared to the execution time of pocketsphinx jobs. From these results, we can
see that there are small but significant inaccuracies in our prediction of
execution time.  However, based on our energy and deadline miss results (see
Figure~\ref{fig:exec_time_prediction.evaluation.em_summary}), the predicted times are adequate
as heuristics for making DVFS decisions without violating deadlines. That is,
the prediction is accurate enough that we are able to differentiate between
short jobs that can be slowed down and long jobs that must be run at high
frequencies. In addition, by skewing the prediction
towards over-predicting, we are able to tolerate these inaccuracies without
causing deadline misses. However, there may be possible energy savings from
improving this prediction accuracy.

In order to explore the possible improvements with perfect prediction, we
implemented an ``oracle'' controller that uses recorded job times from a
previous run with the same inputs to predict the execution time of jobs.
Figure~\ref{fig:exec_time_prediction.evaluation.overhead_comparison} also
includes these oracle results. We see that an additional 11\% energy savings
are achievable with oracle on top of removing the predictor and DVFS switching
overheads. From this, and the prediction errors seen in
Figure~\ref{fig:exec_time_prediction.evaluation.prediction_error}, the most
room for improvement for our prediction-based DVFS controller lies in improving
the prediction accuracy. Note that we do not have oracle results for uzbl and
xpilot as
non-deterministic variations in the ordering of jobs across runs made it
difficult to implement a good oracle controller.

\subsection{Race to Idle}

% Idle power vs. frequency
\begin{figure}
  \begin{center}
    \includegraphics{exec_time_prediction/data/idle_power.pdf}
    \caption{Idle power for the ODROID-XU3's Cortex-A7 core for each frequency
    setting.}
    \label{fig:exec_time_prediction.evaluation.idle_power}
  \end{center}
\end{figure}

It is possible in some situations to optimize energy usage by running quickly
and then dropping to a lower power mode. This is the idea behind ``race to
idle'' techniques.  If a job finishes quickly enough and the idle power is low
enough, then this can save more energy than running for longer at a lower
frequency level.  Figure~\ref{fig:exec_time_prediction.evaluation.idle_power}
shows the idle power at each frequency setting for the ODROID-XU3's Cortex-A7
core.  There is a large difference in idle power at maximum and minimum
frequency, implying that significant energy savings are possible by reducing
the frequency after a job finishes.

% Race to idle
\begin{figure}
  \begin{center}
    \includegraphics{exec_time_prediction/data/em_summary_idle.pdf}
    \caption{Normalized energy usage and deadline misses when the frequency is
    dropped to minimum (200 MHz) when jobs finish.}
    \label{fig:exec_time_prediction.evaluation.em_summary_idle}
  \end{center}
\end{figure}

In an attempt to estimate the possible energy savings from ``race to idle''
techniques, we calculated the energy usage for each controller assuming that
the core only consumes the minimum idle power between jobs.
Figure~\ref{fig:exec_time_prediction.evaluation.em_summary_idle} shows these
energy numbers normalized to the energy usage when always running at maximum
frequency. The performance governor here represents ``race to idle'' where the
max frequency is used during job execution and then the frequency is dropped to
the minimum frequency. We see that this significantly reduces the energy usage
of the performance governor. This technique can also be applied to the other
controllers in order to reduce energy usage. Although the prediction controller
seeks to run as slow as possible in order to finish just before the deadline,
because the prediction is skewed to overestimate, there are cases where jobs
finish early and reducing frequency can be used to further reduce energy usage.
We see that for the benchmarks we tested, our prediction-based controller is
still able to save more energy than the performance and interactive governors.
In some cases, the prediction-based controller uses more energy than the pid
controller. However, as this technique does not affect the number of deadline
misses, the pid controller is still missing many of its deadlines.

\subsection{Under-prediction Trade-off}

% Under-predict trade-off
\begin{figure}
  \begin{center}
    \includegraphics{exec_time_prediction/data/underpredict_sweep.pdf}
    \caption{Energy vs. deadline misses for various under-predict penalty
    weights ($\alpha$) for ldecode.}
    \label{fig:exec_time_prediction.evaluation.underpredict_sweep}
  \end{center}
\end{figure}

In Section~\ref{sec:exec_time_prediction.prediction.model}, we discussed how we
can vary the penalty weight for under-prediction, $\alpha$, when we fit our
execution time prediction model. Placing greater penalty on under-prediction
increases the energy usage but reduces the likelihood of deadline misses.
Figure~\ref{fig:exec_time_prediction.evaluation.underpredict_sweep} shows the
energy and deadline misses for varying under-predict penalty weights for
ldecode. We see that as the weight is decreased, energy consumption decreases
but deadline misses increase. Reducing the weight from 1000 to 100 keep misses
at 0, but reducing the weight to 10 introduces a small number of deadline
misses (0.03\%). Other benchmarks show similar trends and we found that across
the benchmarks we tested, an under-predict penalty weight of 100 provided good
energy savings without sacrificing deadline misses. The results in this chapter
have been shown with a weight of 100.

\subsection{Heterogeneous Cores}

% Motivation for heterogeneous cores
Recent architectures have introduced the use of heterogeneous CPU cores in
order to allow a wider power-performance trade-off range. For example, ARM's
big.LITTLE architecture couples a set of fast out-of-order cores (e.g.,
Cortex-A15) with a set of energy-efficient in-order cores (e.g., Cortex-A7).
These additional operating points can be taken advantage of with our
prediction-based scheme to provide a wider range of operating points, providing
greater energy savings by using slower operating points and/or less deadline
misses by using faster operating points. In this section, we explore the
possible effects of such an architecture using first-order analytical models.
We will focus on an architecture with two different cores (big and little) and
assume that they share an ISA (i.e., programs can run on either without
modification).

% Modification to methodology
The majority of our prediction-based methodology can be applied, unchanged, to
scheduling resources with heterogeneous cores. The first two steps of our
methodology (see
Figure~\ref{fig:exec_time_prediction.prediction.prediction_flow}), generating
program features and predicting execution time, remain unchanged. We modify
the last step, choosing the target frequency, to instead select both core type and
frequency.  This is done by extending our simple linear model of
frequency-based performance scaling of a single core to be a pair of linear
models. For our experiments, we model the big core as a constant factor faster
than the little core.

% Analytical modeling
We created an analytical model to study the first-order effects of having
heterogeneous cores. For each core, execution time scales inversely with
frequency, following the model from
Section~\ref{sec:exec_time_prediction.prediction.dvfs}:
\begin{align*}
  t = T_{mem} + N_{dependent}/f
\end{align*}
As we found that $T_{mem}$ was small for the benchmarks we tested, we set $T_{mem}
= 0$ for our modeling here.

Active power for each core is modeled as
\begin{align*}
  P = \frac{1}{2}CV^2Af
\end{align*}
where $C$ is the capacitance and $A$ is activity factor. With voltage scaling
linearly with respect to frequency (i.e., $V = \delta f$), we have that power
scales cubicly with frequency:
\begin{align*}
  P = \frac{1}{2}CA\delta f^3
\end{align*}

Finally, energy is power multiplied by time:
\begin{align*}
  E &= Pt \\
    &= \left(\frac{1}{2}CA\delta f^3\right) \left(\frac{N_{dependent}}{f}\right) \\
    &= \frac{1}{2}CA\delta N_{dependent}f^2
\end{align*}
We will normalize our energy results to running at maximum frequency:
\begin{align*}
  E_{normalized} = \frac{E}{E_{fmax}} &= \frac{f^2}{f_{max}^2}
\end{align*}
Thus, the values used for the parameters $C$, $A$, $\delta$, and
$N_{dependent}$ are irrelevant.

% Model parameters
\begin{table*}
  \begin{center}
    \begin{footnotesize}
    \input{exec_time_prediction/tabs/model_params.tex}
    \end{footnotesize}
    \caption{Parameters used in DVFS and heterogeneous core model.}
    \label{tab:exec_time_prediction.evaluation.model_params}
  \end{center}
\end{table*}

In order to account for the two heterogeneous cores, we introduce scaling
factors for power and execution time.
\begin{align*}
  P_{little}(f) &= \lambda P_{big}(f) \\
  t_{little}(f) &= \mu t_{big}(f)
\end{align*}
That is, $\lambda$ is the ratio of power between running on the little core and
running on the big core at the same frequency. Similarly, $\mu$ is the ratio
of execution time between running on the little core and on the big core at the
same frequency. The values for these parameters were found through
microbenchmarks on the ODROID-XU3 and are shown in
Table~\ref{tab:exec_time_prediction.evaluation.model_params}. In addition,
Table~\ref{tab:exec_time_prediction.evaluation.model_params} shows the DVFS
levels used for each core. Switching between cores introduces overheads. For
ARM's big.LITTLE architecture, this switching time has been reported to be as
low as 20,000 cycles (\cite{cho-12}) which corresponds to 20 microseconds at 1
GHz. As this switching time is an over an order-of-magnitude lower than the
time budgets we are considering, our first-order model does not account for
switching overheads.

% Basic trade-offs from the model
\begin{figure*}
  \begin{center}
    \begin{subfigure}[Execution time versus frequency]{
      \includegraphics{exec_time_prediction/data/hetero_time_freq.pdf}
      \label{fig:exec_time_prediction.evaluation.hetero_time_freq}
    }
    \end{subfigure}

    \begin{subfigure}[Energy versus frequency]{
      \includegraphics{exec_time_prediction/data/hetero_energy_freq.pdf}
      \label{fig:exec_time_prediction.evaluation.hetero_energy_freq}
    }
    \end{subfigure}

    \begin{subfigure}[Energy versus execution time]{
      \includegraphics{exec_time_prediction/data/hetero_energy_time.pdf}
      \label{fig:exec_time_prediction.evaluation.hetero_energy_time}
    }
    \end{subfigure}
    \caption{Relationship between frequency, execution time, and frequency
    based on analytical model.}
    \label{fig:exec_time_prediction.evaluation.hetero_energy}
  \end{center}
\end{figure*}

Based on this model, we can look at the relative energy and execution time
scaling for operating at different frequencies on each core.
Figure~\ref{fig:exec_time_prediction.evaluation.hetero_time_freq} shows the
normalized execution time for each frequency setting. We can see that at the
same frequency, the big core achieves lower execution time than the little
core. Figure~\ref{fig:exec_time_prediction.evaluation.hetero_energy_freq} shows
the energy for each frequency setting. At the same frequency, the little core
uses less energy than the big core. Finally,
Figure~\ref{fig:exec_time_prediction.evaluation.hetero_energy_time} directly
shows the trade-off between energy and execution time for each core at the
various frequency levels. Only the big core is able to achieve the lowest execution
times. On the other hand, when longer execution times are adequate, the little
core is able to scale out to lower energies. For execution times that are
achievable by both cores, we see that for the model parameters chosen,
the big core is actually more energy-efficient. This trade-off will be
different depending on the model parameters $\lambda$ and $\mu$.

% Evaluation setup
In order to evaluate the effects of heterogeneous cores on actual workloads, we
collected traces of job features and execution times while running at maximum
frequency on the big and little cores of the ODROID-XU3. Using these traces, we
predict the core and frequency to use for each job. Our analytical model is
used to calculate the energy usage of the resulting allocations.

%% Little-only vs big-little, energy and deadline misses
\begin{figure*}
  \begin{center}
    \begin{subfigure}[Normalized budget of 1.0]{
      \includegraphics{exec_time_prediction/data/hetero_little_100_em.pdf}
      \label{fig:exec_time_prediction.evaluation.hetero_little_100_em}
    }
    \end{subfigure}

    \begin{subfigure}[Normalized budget of 0.8]{
      \includegraphics{exec_time_prediction/data/hetero_little_80_em.pdf}
      \label{fig:exec_time_prediction.evaluation.hetero_little_80_em}
    }
    \end{subfigure}

    \begin{subfigure}[Normalized budget of 0.6]{
      \includegraphics{exec_time_prediction/data/hetero_little_60_em.pdf}
      \label{fig:exec_time_prediction.evaluation.hetero_little_60_em}
    }
    \end{subfigure}

    \caption{Energy and deadline misses for running with only a little core
    (littleonly) and with both big and little cores (biglittle).}
    \label{fig:exec_time_prediction.evaluation.hetero_little_em}
  \end{center}
\end{figure*}

% Little-only vs big-little, core/switch counts
\begin{figure*}
  \begin{center}
    \includegraphics{exec_time_prediction/data/hetero_little_counts.pdf}
    \caption{Number of core switches and jobs run on the big core as a
    percentage of total jobs for varying normalized budgets when comparing
    biglittle to littleonly.}
    \label{fig:exec_time_prediction.evaluation.hetero_little_counts}
  \end{center}
\end{figure*}

We first compare the case of running only on a little core versus having both
big and little cores.
Figure~\ref{fig:exec_time_prediction.evaluation.hetero_little_em} shows the
energy and deadline misses for varying time budgets which are normalized to the
worst-case job execution time for each benchmark running on the little core at
max frequency. Energy is normalized to running all jobs at maximum frequency on
the little core and can exceed 100\% due to using the big core. We see that
introducing the big core can actually reduce energy. This is because running on
the big core reduces execution time and leads to reduced energy usage in some cases, as was
shown in Figure~\ref{fig:exec_time_prediction.evaluation.hetero_energy_time}.
For a normalized budget of 1
(Figure~\ref{fig:exec_time_prediction.evaluation.hetero_little_100_em}), we see
an average reduction in normalized energy
of 31\% by introducing the big core. 
For tighter budgets
(Figures~\ref{fig:exec_time_prediction.evaluation.hetero_little_80_em} and
\ref{fig:exec_time_prediction.evaluation.hetero_little_60_em}), the little core
alone is not able to meet all deadlines, showing deadline
misses of 20\% and 35\% for normalized budgets of 0.8 and 0.6 respectively.
Introducing the big core eliminates these deadline misses because of the faster
operating points available.  This can lead to a higher energy usage than when
running on the little core only, as seen for certain benchmarks with a
normalized budget of 0.6.
Figure~\ref{fig:exec_time_prediction.evaluation.hetero_little_counts} shows the
number of 
times that the core type is switched between jobs (e.g., moving from big core
to little core or vice versa) as a percentage of number of jobs. In addition,
it also shows the percentage of jobs run on the big core. For a normalized
budget of 1, 6.6\% of jobs switch cores on average and 78\% of jobs on average
run on the big core.

% Big-only vs big-little, energy and deadline misses
\begin{figure*}
  \begin{center}
    \includegraphics{exec_time_prediction/data/hetero_big_100_em.pdf}
    \caption{Energy and deadline misses for running with only a big core
    (bigonly) and with both big and little cores (biglittle) for a normalized
    budget of 1.}
    \label{fig:exec_time_prediction.evaluation.hetero_big_em}
  \end{center}
\end{figure*}

% Big-only vs big-little, core/switch counts
\begin{figure*}
  \begin{center}
    \includegraphics{exec_time_prediction/data/hetero_big_counts.pdf}
    \caption{Number of core switches and jobs run on the big core as a
    percentage of total jobs for a normalized budget of 1 when comparing
    biglittle to bigonly.}
    \label{fig:exec_time_prediction.evaluation.hetero_big_counts}
  \end{center}
\end{figure*}

% Results bigonly
We also compared the case of running only on a big core versus having big and
little cores. Figure~\ref{fig:exec_time_prediction.evaluation.hetero_big_em} shows
energy and deadline misses for this setup. As the introduction of the little
core is for energy savings as opposed to reducing deadline misses, we only show
results for a normalized budget of 1 which corresponds to the worst-case job
execution time for running on the big core at max frequency. Energy results
are normalized to running all jobs on the big core at max frequency. For some
benchmarks, introducing the little core is able to increase energy savings with
reduction in normalized energy up to 13\%. Introducing the little core does not
affect deadline misses, though we do see a small number of misses for xpilot
and ldecode due to mispredictions.
Figure~\ref{fig:exec_time_prediction.evaluation.hetero_big_counts} shows the
number of core switches and the number of jobs run on the big core as a
percentage of total jobs. On average, 23\% of jobs switch cores and 60\% are
run on the big core.

% Conclusions
In this section, we have shown how our prediction-based methodology can be
extended to architectures with heterogeneous cores. Our first-order results
based on analytical modeling show that with heterogeneous cores, our
prediction-based controller is able to further reduce energy usage and/or
deadline misses. However, there are several additional points to consider for
implementing this on a real system. First, a detailed characterization of the
energy-performance points provided by the target platform are needed.  Here, we
have assumed a constact factor of power and performance difference between the
big and little cores, but this may not hold for a particular system. In
addition, the difference in idle power of the different cores should also be
accounted for. Second, the overhead of switching cores needs to be measured
and, if significant, accounted for in the prediction algorithm. We have
mentioned that the switching time between cores has been reported to be low,
but this does not take into account other possible overheads such as cold cache
misses, pipeline filling, etc. that come with core migration. Finally, in order
to use our prediction-based methodology, a software interface for core
migration will need to be exposed to the controller. Depending on its
implementation, this interface may also introduce switching overheads that need
to be accounted for.

