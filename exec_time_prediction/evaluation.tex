\section{Evaluation}
\label{sec:exec_time_prediction.evaluation}

\subsection{Experimental Setup}
\label{sec:exec_time_prediction.evaluation.setup}

% Benchmark descriptions
\begin{table*}
  \begin{center}
    \begin{footnotesize}
    \input{exec_time_prediction/tabs/benchmarks.tex}
    \end{footnotesize}
    \caption{Benchmark descriptions and task of interest.}
    \label{tab:exec_time_prediction.evaluation.benchmarks}
  \end{center}
\end{table*}

% Benchmark execution times
\begin{table*}
  \begin{center}
    \begin{footnotesize}
    \input{exec_time_prediction/tabs/job_statistics.tex}
    \end{footnotesize}
    \caption{Execution time statistics when running
    at maximum frequency.}
    \label{tab:exec_time_prediction.evaluation.job_statistics}
  \end{center}
\end{table*}

We applied our framework for prediction-based DVFS control to a set of eight
benchmark applications including three games, a web browser, speech
recognition, a video decoder and two applications from the MiBench suite
\cite{mibench}.  Table~\ref{tab:exec_time_prediction.evaluation.benchmarks}
lists and describes these benchmarks and the task of interest for each.
Table~\ref{tab:exec_time_prediction.evaluation.job_statistics} shows the
minimum, average, and maximum job execution times for these benchmarks when run
at maximum frequency.

We ran our experiments on an ODROID-XU3 \cite{odroid} development board running
Ubuntu 14.04. The ODROID-XU3 includes a Samsung Exynos5422 SoC with ARM
Cortex-A15 and Cortex-A7 cores. We show results here for running on the more
power-efficient A7 core but we saw similar trends when running on the A15 core.
In order to isolate measurements for the application of interest, we pinned the
benchmark to run on the A7 core while using the A15 to run OS and background
jobs. We measured power using the on-board power sensors with a sampling rate
of 213 samples per second and integrated over time to calculate energy usage.

We compare our proposed prediction-based DVFS controller with existing
controllers and previously proposed control schemes. Specifically, we measure
results for the following DVFS schemes:
\begin{enumerate}
  \item \textbf{performance}: The Linux performance governor
  \cite{linux_governors} always runs at the maximum frequency. We normalize our
  energy results to this case.
  \item \textbf{interactive}: The Linux interactive governor
  \cite{linux_governors} was designed for interactive mobile applications. It
  samples CPU utilization every 80 milliseconds and changes to maximum
  frequency if CPU utilization is above 85\%.
  \item \textbf{pid}: The PID-based controller uses previous prediction errors
  with a PID control algorithm in order to predict the execution time of the
  next job \cite{gu-dac08}. The PID parameters are trained offline and are
  optimized to reduce deadline misses.
  \item \textbf{prediction}: This is our prediction-based controller as
  described in this chapter.
\end{enumerate}

\subsection{Energy Savings and Deadline Misses}

% Energy, deadline misses
\begin{figure*}
  \begin{center}
    \includegraphics{exec_time_prediction/data/em_summary.pdf}
    \caption{Normalized energy usage and deadline misses.}
    \label{fig:exec_time_prediction.evaluation.em_summary}
  \end{center}
\end{figure*}

Figure~\ref{fig:exec_time_prediction.evaluation.em_summary} compares energy
consumption and deadline misses for the different DVFS controllers across our
benchmark set. These experiments are run with a time budget of 50 milliseconds per job as
running faster than this is not noticeable to a user \cite{lindgaard-bit06,
eqos-hpca15}.  pocketsphinx takes at least 100s of milliseconds to run (see
Table~\ref{tab:exec_time_prediction.evaluation.benchmarks}) so we use a 4
second deadline. This corresponds to the time limit that a user is willing to
wait for a response \cite{miller-afips68}.  Energy numbers are normalized to
the energy usage of the performance governor.  Deadline misses are reported as
the percentage of jobs that miss their deadline. 

We see that, on average, our prediction-based controller saves 56\% energy
compared to running at max frequency. This is 27\% more savings than the
interactive governor and 1\% more savings than the PID-based controller.  For
ldecode, pocketsphinx, and rijndael, prediction-based control shows higher
energy consumption than PID-based control. However, if we look at deadline
misses, we see that PID-based control shows a large number of misses for these
benchmarks. On average, the interactive governor shows 2\% deadline misses and
the PID-based controller shows 13\% misses. In contrast, our prediction-based
controller shows 0.1\% deadline misses for curseofwar and no deadline misses
for the other benchmarks tested. Overall, we see that the interactive governor
has a low number of deadline misses, but achieves this with high energy
consumption. On the other hand, PID-based control shows lower energy usage than
the prediction-based controller in some cases, but this comes at the cost of a
large number of deadline misses. Instead, on average, our prediction-based
control is able to achieve both better energy consumption and less deadline
misses than the interactive governor and PID-based control.

% Deadline sweeps
\begin{figure*}
  \begin{center}
  \begin{subfigure}[2048]{
    \includegraphics{exec_time_prediction/data/2048.pdf}
  }
  \end{subfigure}
  \hspace{-0.3in}
  \begin{subfigure}[curseofwar]{
    \includegraphics{exec_time_prediction/data/curseofwar.pdf}
  }
  \end{subfigure}

  \begin{subfigure}[ldecode]{
    \includegraphics{exec_time_prediction/data/ldecode.pdf}
  }
  \end{subfigure}
  \hspace{-0.3in}
  \begin{subfigure}[pocketsphinx]{
    \includegraphics{exec_time_prediction/data/pocketsphinx.pdf}
  }
  \end{subfigure}

  \begin{subfigure}[rijndael]{
    \includegraphics{exec_time_prediction/data/rijndael.pdf}
  }
  \end{subfigure}
  \hspace{-0.3in}
  \begin{subfigure}[sha]{
    \includegraphics{exec_time_prediction/data/sha.pdf}
  }
  \end{subfigure}

  \begin{subfigure}[uzbl]{
    \includegraphics{exec_time_prediction/data/uzbl.pdf}
  }
  \end{subfigure}
  \hspace{-0.3in}
  \begin{subfigure}[xpilot]{
    \includegraphics{exec_time_prediction/data/xpilot.pdf}
  }
  \end{subfigure}

  \caption{Normalized energy usage and deadline misses as time budget is
  varied.}
  \label{fig:exec_time_prediction.evaluation.sweeps}
  \end{center}
\end{figure*}

Since our prediction-based controller takes the time budget into account, it is
able to save more energy with longer time budgets. Similarly, with shorter time
budgets, it will spend more energy to attempt to meet the tighter deadlines.
In order to study this trade-off, we swept the time budget around the point
where we expect to start seeing deadline misses. Specifically, we set a
normalized budget of 1 to correspond to the maximum execution time seen for the
task when running at maximum frequency (see
Table~\ref{tab:exec_time_prediction.evaluation.benchmarks}). This corresponds
to the tightest budget such that all jobs are able to meet their deadline.
Figure~\ref{fig:exec_time_prediction.evaluation.sweeps} shows the energy usage
and deadline misses for the various benchmarks as the normalized budget is
swept below and above 1.  We see that our prediction-based controller is able
to save more energy with longer time budgets and continues to outperform the
interactive governor and the PID-based controller for varying time budgets.
For normalized budgets less than 1, our prediction-based controller shows
deadline misses. However, the number of misses is typically close to the number
seen with the performance governor.  This implies that the only deadline misses
are the ones that are impossible to meet at the specified time budget, even
with running at the maximum frequency.

\subsection{Analysis of Overheads and Error}
\label{sec:exec_time_prediction.evaluation.overheads}

% Overhead time
\begin{figure}
  \begin{center}
    \includegraphics{exec_time_prediction/data/overhead_time.pdf}
    \caption{Average time to run prediction slice and switch DVFS levels.}
    \label{fig:exec_time_prediction.evaluation.overhead_time}
  \end{center}
\end{figure}

Figure~\ref{fig:exec_time_prediction.evaluation.overhead_time} shows the
average times for executing the predictor and for switching DVFS levels.  On
average, the predictor takes 3.2 ms to execute and DVFS switching takes 0.8 ms.
Excluding pocketsphinx, the average total overhead is less than 1 ms which is
2\% of a 50 ms time budget.  pocketsphinx shows a long execution time for the
predictor. However, this time is negligible compared to the execution time of
pocketsphinx jobs which are on the order of seconds.

% Results w/o overhead
\begin{figure}
  \begin{center}
    \includegraphics{exec_time_prediction/data/overhead_comparison.pdf}
    \caption{Normalized energy usage and deadline misses with overheads removed
    and oracle prediction.}
    \label{fig:exec_time_prediction.evaluation.overhead_comparison}
  \end{center}
\end{figure}

The overheads of executing the predictor and DVFS switching decrease the energy
savings achievable. This is due to the energy consumed to perform these
operations as well as the decrease in effective budget.  Better program slicing
and/or feature selection could reduce the predictor execution time.  Similarly,
faster DVFS switching circuits \cite{booster-hpca12, shortstop-vlsic13,
fgsync-micro14} have shown switching times on the order of tens of nanoseconds.
In order to explore the limits of what is achievable, we evaluated our
prediction-based control when the overheads of the predictor and DVFS switching
are ignored.
Figure~\ref{fig:exec_time_prediction.evaluation.overhead_comparison} shows the
energy and deadline misses when these overheads are removed. These results are
shown for a time budget of 4 seconds for pocketsphinx and 50 milliseconds for all other
benchmarks.  On average, we see a 3\% decrease in energy consumption when
removing the overheads of DVFS switching. Removing the overhead of running the
predictor shows negligible improvement past removing the DVFS switching
overhead.

% Prediction error
\begin{figure}
  \begin{center}
    \includegraphics{exec_time_prediction/data/prediction_error.pdf}
    \caption{Box-and-whisker plots of prediction error. Positive number
    correspond to over-prediction and negative numbers correspond to
    under-prediction.}
    \label{fig:exec_time_prediction.evaluation.prediction_error}
  \end{center}
\end{figure}

In addition to these overheads, the accuracy of our prediction limits the
effectiveness of the prediction-based controller.
Figure~\ref{fig:exec_time_prediction.evaluation.prediction_error} shows
box-and-whisker plots of the prediction error.  The box indicates the first and
third quartiles and the line in the box marks the median value. Outliers
(values over 1.5 times the inner quartile range past the closest box end) are
marked with an ``x'' and the non-outlier range is marked by the whiskers.
Positive values represent over-prediction and negative-numbers represent
under-prediction. We can see that the prediction skews toward over-prediction
with average errors greater than 0.  Most benchmarks show prediction errors of
less than 5 ms, which is only 10\% of a 50 ms time budget. ldecode and rijndael
show higher prediction errors, which limits the energy savings possible.
pocketsphinx (not shown) has errors ranging from 60 ms under-prediction to 2
seconds over-prediction with an average of 880 ms over-prediction. Although
these errors are larger in absolute terms than the other benchmarks, they are
on the same order of magnitude when when compared to the execution time of
pocketsphinx jobs.

In order to explore the possible improvements with perfect prediction, we
implemented an ``oracle'' controller that uses recorded job times from a
previous run with the same inputs to predict the execution time of jobs.
Figure~\ref{fig:exec_time_prediction.evaluation.overhead_comparison} also
includes these oracle results. We see than an additional 11\% energy savings
are achievable with oracle on top of removing the predictor and DVFS switching
overheads. Note that we do not have oracle results for uzbl and xpilot as
non-deterministic variations in the ordering of jobs across runs made it
difficult to implement a good oracle controller.

\subsection{Under-prediction Trade-off}

% Under-predict trade-off
\begin{figure}
  \begin{center}
    \includegraphics{exec_time_prediction/data/underpredict_sweep.pdf}
    \caption{Energy vs. deadline misses for various under-predict penalty
    weights ($\alpha$) for ldecode.}
    \label{fig:exec_time_prediction.evaluation.underpredict_sweep}
  \end{center}
\end{figure}

In Section~\ref{sec:exec_time_prediction.prediction.model}, we discussed how we
can vary the penalty weight for under-prediction, $\alpha$, when we fit our
execution time prediction model. Placing greater penalty on under-prediction
increases the energy usage but reduces the likelihood of deadline misses.
Figure~\ref{fig:exec_time_prediction.evaluation.underpredict_sweep} shows the
energy and deadline misses for varying under-predict penalty weights for
ldecode. We see that as the weight is decreased, energy consumption decreases
but deadline misses increase. Reducing the weight from 1000 to 100 keep misses
at 0, but reducing the weight to 10 introduces a small number of deadline
misses (0.03\%). Other benchmarks show similar trends and we found that across
the benchmarks we tested, an under-predict penalty weight of 100 provided good
energy savings without sacrificing deadline misses. The results in this chapter
have been shown with a weight of 100.

\subsection{Heterogeneous Cores}

% Motivation for heterogeneous cores
Recent architectures have introduced the use of heterogeneous CPU cores in
order to allow a wider power-performance trade-off range. For example, ARM's
big.LITTLE architecture couples a set of fast out-of-order cores (e.g.,
Cortex-A15) with a set of energy-efficient in-order cores (e.g., Cortex-A7).
These additional operating points can be taken advantage of with our
prediction-based to provide a wider range of operating points, providing
greater energy savings by using slower operating points and/or less deadline
misses by using faster operating points. In this section, we explore the
possible effects of such an architecture using first-order analytical models.
We will focus on an architecture with two different cores (big and little) and
assume that they share an ISA (i.e., programs can run on either without
modification).

% Modification to methodology
The majority of our prediction-based methodology can be applied, unchanged, to
scheduling resources with heterogeneous cores. The first two steps of our
methodology (see
Figure~\ref{fig:exec_time_prediction.prediction.prediction_flow}), generating
program features and predicting execution time , remain unchanged. We modify
the last step, predicting frequency, to instead predict both core type and
frequency.  This is done by extending our simple linear model of
frequency-based performance scaling of a single core to be a pair of linear
models. For our experiments, we model the big core as a constant factor faster
than the little core.

% Analytical modeling
We created an analytical model to study the first-order effects of having
heterogeneous cores. For each core, execution time scales inversely with
frequency, following the model from
Section~\ref{sec:exec_time_prediction.prediction.dvfs}:
\begin{align*}
  t = T_{mem} + N_{dependent}/f
\end{align*}
As we found that $T_{mem}$ was small for the benchmarks we tested, we set $T_{mem}
= 0$ for our modeling here.

Active power for each core is modeled as
\begin{align*}
  P = \frac{1}{2}CV^2Af
\end{align*}
where $C$ is the capacitance and $A$ is activity factor. With voltage scaling
linearly with respect to frequency (i.e., $V = \delta f$), we have that power
scales cubicly with frequency:
\begin{align*}
  P = \frac{1}{2}CA\delta f^3
\end{align*}

Finally, energy is power multiplied by time:
\begin{align*}
  E &= Pt \\
    &= \left(\frac{1}{2}CA\delta f^3\right) \left(\frac{N_{dependent}}{f}\right) \\
    &= \frac{1}{2}CA\delta N_{dependent}f^2
\end{align*}
We will normalize our energy results to running at maximum frequency:
\begin{align*}
  E_{normalized} = \frac{E}{E_{fmax}} &= \frac{f^2}{f_{max}^2}
\end{align*}
Thus, the values used for the parameters $C$, $A$, $\delta$, and
$N_{dependent}$ are irrelevant.

% Model parameters
\begin{table*}
  \begin{center}
    \begin{footnotesize}
    \input{exec_time_prediction/tabs/model_params.tex}
    \end{footnotesize}
    \caption{Parameters used in DVFS and heterogeneous core model.}
    \label{tab:exec_time_prediction.evaluation.model_params}
  \end{center}
\end{table*}

In order to account for the two heterogeneous cores, we introduce scaling
factors for power and execution time.
\begin{align*}
  P_{little}(f) &= \lambda P_{big}(f) \\
  t_{little}(f) &= \mu t_{big}(f)
\end{align*}
That is, $\lambda$ is the ratio of power between running on the little core and
running on the big core at the same frequency. Similarly, $\mu$ is the ratio
of execution time between running on the little core and on the big core at the
same frequency. The values for these parameters were found through
microbenchmarks on the ODROID-XU3 and are shown in
Table~\ref{tab:exec_time_prediction.evaluation.model_params}. In addition,
Table~\ref{tab:exec_time_prediction.evaluation.model_params} shows the DVFS
levels used for each core. Switching between cores introduces overheads. For
ARM's big.LITTLE architecture, this switching time has been reported to be as
low as 20,000 cycles (\cite{cho-12}) which corresponds to 20 microseconds at 1
GHz. As this time is an over an order-of-magnitude lower than the times we are
considering, our first-order model does not account for switching overheads.

% Basic trade-offs from the model
\begin{figure*}
  \begin{center}
    \begin{subfigure}[Execution time versus frequency]{
      \includegraphics{exec_time_prediction/data/hetero_time_freq.pdf}
      \label{fig:exec_time_prediction.evaluation.hetero_time_freq}
    }
    \end{subfigure}

    \begin{subfigure}[Energy versus frequency]{
      \includegraphics{exec_time_prediction/data/hetero_energy_freq.pdf}
      \label{fig:exec_time_prediction.evaluation.hetero_energy_freq}
    }
    \end{subfigure}

    \begin{subfigure}[Energy versus execution time]{
      \includegraphics{exec_time_prediction/data/hetero_energy_time.pdf}
      \label{fig:exec_time_prediction.evaluation.hetero_energy_time}
    }
    \end{subfigure}
    \caption{Relationship between frequency, execution time, and frequency
    based on analytical model.}
    \label{fig:exec_time_prediction.evaluation.hetero_energy}
  \end{center}
\end{figure*}

Based on this model, we can look at the relative energy and execution time
scaling for operating at different frequencies on each core.
Figure~\ref{fig:exec_time_prediction.evaluation.hetero_time_freq} shows the
normalized execution time for each frequency setting. We can see that at the
same frequency, the big core achieves lower execution time than the little
core. Figure~\ref{fig:exec_time_prediction.evaluation.hetero_energy_freq} shows
the energy for each frequency setting. At the same frequency, the little core
uses less energy than the big core. Finally,
Figure~\ref{fig:exec_time_prediction.evaluation.hetero_energy_time} directly
shows the trade-off between energy and execution time for each core at the
various frequency levels. Only the big core is able to achieve low execution
times. On the other hand, when longer execution times are adequate, the little
core is able to scale out to lower energies. For execution times that are
achievable by both cores, we actually see that for the model parameters chosen,
the big core is actually more energy-efficient. This trade-off will be
different depending on the model parameters $\lambda$ and $\mu$.

% Evaluation setup
In order to evaluate the effects of heterogeneous cores on actual workloads, we
collected traces of job features and execution times while running at maximum
frequency on the big and little cores of the ODROID-XU3. Using these traces, we
predict the core and frequency to use for each job. Our analytical model is
used to calculate the energy usage of the resulting allocations.

% Little-only vs big-little, energy and deadline misses
\begin{figure*}
  \begin{center}
    \includegraphics{exec_time_prediction/data/hetero_little_em.pdf}
    \caption{Energy and deadline misses for running with only a little core
    (littleonly-x) and with both big and little cores (biglittle-x) where x is
    the normalized budget.}
    \label{fig:exec_time_prediction.evaluation.hetero_little_em}
  \end{center}
\end{figure*}

% Little-only vs big-little, core/switch counts
\begin{figure*}
  \begin{center}
    \includegraphics{exec_time_prediction/data/hetero_little_counts.pdf}
    \caption{Number of core switches and jobs run on the big core as a
    percentage of total jobs for varying normalized budgets.}
    \label{fig:exec_time_prediction.evaluation.hetero_little_counts}
  \end{center}
\end{figure*}

We first compare the case of running only on a little core versus having both
big and little cores.
Figure~\ref{fig:exec_time_prediction.evaluation.hetero_little_em} shows the
energy and deadline misses for varying budgets which are normalized to the
worst-case job execution time for each benchmark running on the little core at
max frequency. Energy is normalized to running all jobs at maximum frequency on
the little core and can exceed 100\% due to using the big core. We see that
introducing the big core can actually reduce energy. This is because running on
the big core reduces execution time and leads to reduced energy usage, as was
shown in Figure~\ref{fig:exec_time_prediction.evaluation.hetero_energy_time}.
For a normalized budget of 1, we see an average reduction in normalized energy
of 31\% by introducing the big core. In addition, for tighter budgets, the
little core alone is not able to meet all deadlines, showing deadline misses of
35\% and 20\% for normalized budgets of 0.6 and 0.8 respectively. Introducing
the big core eliminates these deadline misses because of the faster operating
points available. In some cases, this leads to higher overall energy usage.
Figure~\ref{fig:exec_time_prediction.evaluation.hetero_little_counts} shows
times that the core type is switched between jobs (e.g., moving from big core
to little core or vice versa) as a percentage of number of jobs. In addition,
it also shows the percentage of jobs run on the big core. For a normalized
budget of 1, 6.6\% of jobs switch cores on average and 78\% of jobs on average
run on the big core.

% Big-only vs big-little, energy and deadline misses
\begin{figure*}
  \begin{center}
    \includegraphics{exec_time_prediction/data/hetero_big_em.pdf}
    \caption{Energy and deadline misses for running with only a big core and
    with both big and little cores for a normalized budget of 1.}
    \label{fig:exec_time_prediction.evaluation.hetero_little_em}
  \end{center}
\end{figure*}

% Big-only vs big-little, core/switch counts
\begin{figure*}
  \begin{center}
    \includegraphics{exec_time_prediction/data/hetero_big_counts.pdf}
    \caption{Number of core switches and jobs run on the big core as a
    percentage of total jobs for a normalized budget of 1.}
    \label{fig:exec_time_prediction.evaluation.hetero_big_counts}
  \end{center}
\end{figure*}

% Results bigonly
We also compared the case of running only on a big core versus having big and
little cores. Figure~\ref{fig:exec_time_prediction.evaluation.bigonly} shows
energy and deadline misses for this setup. As the introduction of the little
core is for energy savings as opposed to reducing deadline misses, we only show
results for a normalized budget of 1 which corresponds to the worst-case job
execution time for running on the big core at max frequency. Energy results
are normalized to running all jobs on the big core at max frequency. For some
benchmarks, introducing the little core is able to increase energy savings with
reduction in normalized energy up to 13\%. Introducing the little core does not
affect deadline misses, though we do see a small number of misses for xpilot
and ldecode due to mispredictions.
Figure~\ref{fig:exec_time_prediction.evaluation.hetero_big_counts} shows the
number of core switches and the number of jobs run on the big core as a
percentage of total jobs. On average, 23\% of jobs switch cores and 60\% are
run on the big core.

% Conclusions
These results show that improved energy savings and/or reduced deadline misses
can be achieved by taking advantage of heterogeneous cores. In addition, our
prediction-based methodology can be easily extended to select operating points
on such a system.
