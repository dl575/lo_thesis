\section{Execution Time Prediction for Energy-Efficiency}
\label{sec:exec_time_prediction.prediction}

\subsection{Overview}
\label{sec:exec_time_prediction.prediction.overview}

% Control flow graph
\begin{figure}
  \begin{center}
    \includegraphics{exec_time_prediction/figs/cfg.pdf}
    \caption{Example control flow graph. Each node is annotated with its number
    of instructions.}
    \label{fig:exec_time_prediction.prediction.cfg}
  \end{center}
\end{figure}

The basic intuition behind our prediction methodology is that, to first-order,
execution time correlates with the number of instructions run. Variations in
the number of instructions run are described by the control flow taken by a
specific job. For example, consider the control flow graph for a task shown in
Figure~\ref{fig:exec_time_prediction.prediction.cfg}. Each node is marked with
its number of instructions.  Taking the left branch instead of the right branch
corresponds to nine more instructions being executed. Similarly, each
additional loop iteration of the last basic block adds five instructions to the
number of instructions executed. By knowing which branch is taken and the
number of loop iterations, we can know the number of instructions executed and
estimate the execution time.  With an estimate of the execution time, we can
then estimate the performance-scaling impact of DVFS and choose an appropriate
frequency and voltage level to run at in order to just meet the deadline.

% Prediction flow
\begin{figure}
  \begin{center}
    \includegraphics{exec_time_prediction/figs/prediction_flow.pdf}
    \caption{Steps to predict execution time from job input and program state.}
    \label{fig:exec_time_prediction.prediction.prediction_flow}
  \end{center}
\end{figure}

Figure~\ref{fig:exec_time_prediction.prediction.prediction_flow} shows the main
steps in our prediction method. We first instrument the task source code and
use program slicing to create a code fragment that will calculate control flow
features for a job. The code fragment is run before a job executes in order to
generate the control flow features
(Section~\ref{sec:exec_time_prediction.prediction.features}). Next, we use a
linear model, which we train off-line, to map control flow features to an
execution time estimate for the job
(Section~\ref{sec:exec_time_prediction.prediction.model}). Finally, we use
classical linear models \cite{xie-pldi03, wu-micro05} that describe the
frequency-performance trade-off of DVFS to select an appropriate frequency
(Section~\ref{sec:exec_time_prediction.prediction.dvfs}).

\subsection{Program Features}
\label{sec:exec_time_prediction.prediction.features}

% Example insertion of feature counters
\begin{figure}
  \begin{center}
    \includegraphics{exec_time_prediction/figs/features.pdf}
    \caption{Example of feature counters inserted for conditionals, loops, and
    function calls.}
    \label{fig:exec_time_prediction.prediction.features}
  \end{center}
\end{figure}

The first step needed for our prediction is to generate control flow features.
That is, we want to know the control flow of a task when executing with a
specific input and program state.  For this purpose, we instrument the task
source to count these control flow features.  Specifically, we instrument the
task to count the following features:
\begin{itemize}
  \item Number of iterations for each loop
  \item Number of times each conditional branch is taken
  \item Address of each function pointer call
\end{itemize}
Figure~\ref{fig:exec_time_prediction.prediction.features} shows examples of how
these features are instrumented.  We focus on control flow features because
these explain most of the execution time variation, especially over a large
number of instructions. However, other features,
such as variable values or memory accesses, could be included to improve the
prediction accuracy.

% Add loop counters, slice
\begin{figure*}
  \begin{center}
    \includegraphics{exec_time_prediction/figs/code_transformations.pdf}
    \caption{Example of program slicing for control flow features.}
    \label{fig:exec_time_prediction.prediction.code_transformations}
  \end{center}
\end{figure*}

Generating these features using an instrumented version of the task code is not
suitable for prediction because the instrumented task will take at least as
long as the original task to run. Instead, we need to quickly generate these
features before the task execution. In order to minimize the prediction
execution time, we use program slicing \cite{tip-jpl95} to produce the minimal
code needed to calculate these features.
Figure~\ref{fig:exec_time_prediction.prediction.code_transformations} shows a
simple example of this flow. By removing the actual computation and only
running through the control flow, the execution time can be greatly reduced. 
Note that since the information from this slice will ultimately be used to make a
heuristic decision on DVFS control, we do not need an exact slice
that perfectly calculates the program features. Issues such as pointer aliasing
can limit the amount of code removed for an exact slice.  Instead, using an
approximate slice can reduce the slice's size and execution time. As long as
inaccuracies in the generated program features are low, an approximate slice is
adequate for our prediction needs.
We refer to the resulting program slice that computes the control flow features
as the \emph{prediction slice} or simply as the \emph{slice}.
%An exact program slice may not be much smaller than the original program as
%issues such as pointer aliasing can require large portions of the original
%program to remain in the slice. Thus, in order to keep the execution time of
%the prediction slice low, we use an approximate slice. As we will ultimately
%use these features to make a heuristic decision on DVFS control, we can
%tolerate some inaccuracy in the feature values.

One problem that arises with running this prediction slice before a task is the
issue of side-effects. That is, the slice could write to global variables and
break the correctness of the program. In order to prevent this, the slice
creates local copies of any global variables that are used. Values for these
local copies are updated at the start of the slice and writes are only applied
to the local copy. A similar process is applied to any arguments that are
passed by reference.

\subsection{Execution Time Prediction Model}
\label{sec:exec_time_prediction.prediction.model}

% Runtime power of monitoring core
\begin{table}[tb]
  \begin{center}
    \begin{small}
    \input{exec_time_prediction/tabs/variables.tex}
    \end{small}
    \caption{Variable and notation descriptions.}
    \label{tab:exec_time_prediction.prediction.variables}
  \end{center}
\end{table}

Next, we need to predict the execution time from the control flow features.
This section describes our model that maps features to execution time.
Table~\ref{tab:exec_time_prediction.prediction.variables} summarizes the
variables and notation that are used in this section.  We use a linear model to
map features to execution time as this captures the basic correlation.
Higher-order or non-polynomial models may provide better accuracy.  However, a
linear model has the advantage of being both simple to train and fast to
evaluate at run-time. In addition, it is always convex which allows us to use
convex optimization-based methods to fit the model.  Our linear model can be
expressed as
\begin{align*}
  \bar{y} = \mathbf{x} \boldsymbol{\beta}
\end{align*}
where $\bar{y}$ is the predicted execution time, $\mathbf{x}$ is a vector of
feature values, and $\boldsymbol{\beta}$ are the coefficients that map feature
values to execution time. These $\boldsymbol{\beta}$ coefficients are fit using
profiling data. Specifically, we profile the program to produce a set of
training data consisting of execution times $\mathbf{y}$ and feature vectors
$\mathbf{X}$ (i.e., each row of $\mathbf{X}$ is a vector of features,
$\mathbf{x}_i$, for one job). Note that in order to achieve the expected linear
correlation between features and execution time, addresses recorded for
function calls are converted to a one-hot encoding indicating whether
particular function addresses were called or not.

The most common way to fit a linear model is to use linear least squares
regression.  Linear least squares regression finds the coefficients
$\boldsymbol{\beta}$ that minimize the mean square error:
\begin{align*}
\begin{aligned}
  \underset{\boldsymbol{\beta}}{\text{argmin}} & & \|\mathbf{X}\boldsymbol{\beta} - \textbf{y}\|^2
\end{aligned}
\end{align*}
Essentially, this aims to minimize the sum of the absolute errors in the
prediction. That is, it weights negative and positive errors equally. However,
these two errors lead to different behaviors on our system.  Negative errors
(under-prediction) lead to deadline misses since we predict the job to run
faster than its actual execution time. On the other hand, positive errors
(over-prediction) result in an overly conservative frequency setting which does
not save as much energy as possible. In order to maintain a good user
experience, we would prefer to avoid deadline misses, possibly at the cost of
energy usage. In other words, we should place greater weight on avoiding
under-prediction as opposed to over-prediction.

We can place greater weight on under-prediction by modifying our optimization
objective:
\begin{align*}
\begin{aligned}
  \underset{\boldsymbol{\beta}}{\text{argmin}} & & \|pos(\textbf{X}\boldsymbol{\beta} - \textbf{y})\|^2 + \alpha \|neg(\textbf{X}\boldsymbol{\beta} - \textbf{y})\|^2
\end{aligned}
\end{align*}
where $pos(x) = max\{x, 0\}$ and $neg(x) = max\{-x, 0\}$ and these functions
are applied element-wise to vectors. Thus, $\|pos(\textbf{X}\boldsymbol{\beta}
- \textbf{y})\|^2$ represents the over-prediction error and
$\|neg(\textbf{X}\boldsymbol{\beta} - \textbf{y})\|^2$ represents the
under-prediction error. $\alpha$ is a weighting factor that allows us to place
a greater penalty on under-predictions by setting $\alpha > 1$.  Since this
objective is convex, we can use existing convex optimization solvers to solve
for $\boldsymbol{\beta}$.

Coefficients which are zero imply that the corresponding control flow features
do not need to be calculated by the prediction slice. We can use this
information to further reduce the size and execution time of the prediction
slice. We can extend our optimization objective to favor using less features by
using the Lasso method \cite{lasso-jrss96}:
\begin{align*}
\begin{aligned}
  \underset{\boldsymbol{\beta}}{\text{argmin}} & & \|pos(\textbf{X}\boldsymbol{\beta} - \textbf{y})\|^2 + \alpha \|neg(\textbf{X}\boldsymbol{\beta} - \textbf{y})\|^2 + \gamma \|\boldsymbol{\beta}\|_1
\end{aligned}
\end{align*}
where $\|\cdot\|_1$ is the L1-norm and $\gamma$ is a weighting factor that
allows us to trade off prediction accuracy with the number of features needed.

\subsection{DVFS Model}
\label{sec:exec_time_prediction.prediction.dvfs}

% DVFS linearity
\begin{figure}
  \begin{center}
    \includegraphics{exec_time_prediction/figs/dvfs_linearity.pdf}
    \caption{Average execution time of jobs (frames) for ldecode (video
    decoding) as frequency level varies.}
    \label{fig:exec_time_prediction.prediction.dvfs_linearity}
  \end{center}
\end{figure}

Given a predicted execution time, we need to estimate how the execution time
will change with varying frequency. For this, we use the classical linear model
found in literature \cite{xie-pldi03, wu-micro05}:
\begin{align*}
  t = T_{mem} + N_{dependent}/f
\end{align*}
where $t$ is the execution time, $T_{mem}$ is the memory-dependent execution
time that does not scale with frequency, $N_{dependent}$ is the number of CPU
cycles that do not overlap with memory and scale with frequency, and $f$ is the
frequency.  In order to verify this linearity assumption, we ran our benchmark
applications at each available frequency level.  We then tested the linear
correlation between average job time and frequency for each application.
Figure~\ref{fig:exec_time_prediction.prediction.dvfs_linearity} shows the
average job execution time versus $1/f$ for ldecode (video decoder
application). We can see that $t$ and $1/f$ do show a linear relationship. 
All applications we tested showed a linear relationship between execution time
and $1/f$ with squared correlation coefficient $r^2 > 0.99$.

For this linear model, by predicting the execution time at two points, we can
determine $T_{mem}$ and $N_{dependent}$ for a job and calculate the minimum
frequency $f$ to satisfy a given time budget $t_{budget}$. More specifically,
we predict the execution time $\bar{t}_{fmin}$ at minimum frequency $f_{min}$
and the execution time $\bar{t}_{fmax}$ at maximum frequency $f_{max}$. Using
these two points, we can calculate $T_{mem}$ and $N_{dependent}$ as
\begin{align*}
  N_{dependent} &= \frac{f_{min}f_{max}(\bar{t}_{fmin} - \bar{t}_{fmax})}{f_{max} - f_{min}} \\
  T_{mem} &= \frac{f_{max}\bar{t}_{fmax} - f_{min}\bar{t}_{fmin}}{f_{max} - f_{min}}
\end{align*}

For a given budget $t_{budget}$, we want the minimum frequency $f_{budget}$
that will meet this time. This can be calculated as
\begin{align*}
  f_{budget} = \frac{N_{dependent}}{t_{budget} - T_{mem}}
\end{align*}

Since execution time can vary even with the same job inputs and program state,
we add a margin to the predicted execution times used ($t_{fmin}$ and
$t_{fmax}$). In our experiments we used a margin of 10\%. A higher margin can
decrease deadline misses while a lower margin can improve the energy savings.
The resulting predicted frequency is the exact frequency that we expect will
just satisfy the time budget. However, DVFS is only supported for a set of
discrete frequency levels. Thus, the actual frequency we select is the smallest
frequency allowed that is greater than $f_{budget}$. 

% Effective budget
\begin{figure}
  \begin{center}
    \includegraphics{exec_time_prediction/figs/effective_budget.pdf}
    \caption{The effective budget decreases due to slice and DVFS execution time.}
    \label{fig:exec_time_prediction.prediction.effective_budget}
  \end{center}
\end{figure}

% DVFS switching time
\begin{figure}
  \begin{center}
    \includegraphics{exec_time_prediction/data/dvfs_heatmap.pdf}
    \caption{95th-percentile switching times for DVFS.}
    \label{fig:exec_time_prediction.prediction.dvfs_heatmap}
  \end{center}
\end{figure}

The execution of the prediction slice and DVFS switch reduces the amount of
time available for a job to execute and still satisfy its budget. Thus, the
effective budget when choosing a frequency to run at needs to consider these
overheads (see
Figure~\ref{fig:exec_time_prediction.prediction.effective_budget}). Although
the execution time of the prediction slice can be measured, the DVFS switching
time must be estimated, as the switch has not been performed yet.  This is done
by microbenchmarking the DVFS switching time.
Figure~\ref{fig:exec_time_prediction.prediction.dvfs_heatmap} shows the
95th-percentile DVFS switching times for our test platform for each possible
start and ending frequency. We use the 95th-percentile switching times in order
to be conservative in our estimate of DVFS switching time while omitting rare
outliers. 

\subsection{Alternate Prediction Models}
\label{sec:exec_time_prediction.prediction.alternate_models}

In this section, we have described our specific prediction strategy for each
step in our overall prediction flow shown in
Figure~\ref{fig:exec_time_prediction.prediction.prediction_flow}. However, we
note that each step in this prediction flow can be substituted with alternate
models as long as it produces the needed prediction for the next step.  The
most obvious change would be to use more complex prediction models for each
step (e.g., more features generated and higher-order, non-linear models) in
order to improve the prediction accuracy.  
%For the benchmarks we evaluated, we saw relatively little gain to be had from
%improved prediction (see
%Section~\ref{sec:exec_time_prediction.evaluation.overheads}) and thus the
%increased overheads of more complex models were not justified.  
Here, we discuss some other possible directions for extending our prediction flow.

For feature generation, we have focused on automated generation in order for
the approach to be general and limit the need for domain-specific expertise.
However, this does not preclude the programmer from manually adding ``hints''
that they expect would correlate well with a job's execution time. For example,
the programmer may be able to extract metadata from input files and manually
encode these to features.

One interesting extension to execution time prediction involves its use in
feature selection. Additional constraints could be added to the execution time
prediction in order to limit the use of features which require high overhead to
generate. Features over some overhead threshold could be explicitly disallowed
or the overhead for each feature could be introduced as penalties in the
optimization objective.

The last step in our flow focuses on selecting an appropriate frequency level
for DVFS control. However, this last step could be substituted to support other
performance-energy trade-off mechanisms, such as heterogeneous cores. By using
alternate models for how the execution time scales with the performance-energy
trade-off mechanism, an appropriate operating point can be selected for the
mechanism of interest.
